name: shopintel-local

services:
  # ─── PostgreSQL: relational truth store ───
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: shop
      POSTGRES_USER: shop_user
      POSTGRES_PASSWORD: shop_pass_local
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/migrations:/docker-entrypoint-initdb.d:ro
    ports:
      - "127.0.0.1:5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U shop_user -d shop"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  # ─── FalkorDB: graph store ───
  falkordb:
    image: falkordb/falkordb:latest
    volumes:
      - falkordbdata:/data
    ports:
      - "127.0.0.1:6380:6379"
      - "127.0.0.1:3002:3000"
    healthcheck:
      test: ["CMD", "redis-cli", "PING"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  # ─── Backend: Fastify + QueryWeaver ───
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
      falkordb:
        condition: service_healthy
      llm:
        condition: service_started
    environment:
      PORT: 8080
      NODE_ENV: production
      POSTGRES_URL: postgresql://shop_user:shop_pass_local@postgres:5432/shop
      FALKORDB_URL: redis://falkordb:6379
      FALKOR_URL: redis://falkordb:6379
      GRAPH_NAME: shop
      OPENAI_BASE_URL: http://llm:11434/v1
      OPENAI_API_KEY: ollama
      LLM_MODEL_ID: llama3.2
      LLM_TIMEOUT_MS: "180000"
      LLM_RETRIES: "2"
      OFFLINE_ONLY: "true"
      QUERYWEAVER_CONFIG_PATH: ./config/queryweaver.config.json
      AUDIT_LOG_DIR: /app/audit
    ports:
      - "127.0.0.1:8090:8080"
    volumes:
      - audit_logs:/app/audit
    restart: unless-stopped

  # ─── Frontend: Wizechat UI ───
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    depends_on:
      - backend
    ports:
      - "127.0.0.1:3001:80"
    restart: unless-stopped

  # ─── Local LLM: Ollama (OpenAI-compatible, runs on CPU/Apple Silicon/GPU) ───
  llm:
    image: ollama/ollama:latest
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  # ─── For DGX Spark with NVIDIA GPU, replace above with NIM: ───
  # llm-nim:
  #   image: nvcr.io/nim/meta/llama-3.1-8b-instruct:latest
  #   runtime: nvidia
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   ports:
  #     - "127.0.0.1:8000:8000"
  #   volumes:
  #     - nim_cache:/opt/nim/.cache
  #   environment:
  #     - NGC_API_KEY=${NVIDIA_API_KEY:-no-key}

volumes:
  pgdata:
  falkordbdata:
  audit_logs:
  ollama_data:
  # nim_cache:
