name: shopintel-local

services:
  # ─── PostgreSQL: relational truth store ───
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: shop
      POSTGRES_USER: shop_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-shop_pass_local}
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./db/migrations:/docker-entrypoint-initdb.d:ro
    ports:
      - "127.0.0.1:5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U shop_user -d shop"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  # ─── FalkorDB: graph store ───
  falkordb:
    image: falkordb/falkordb:latest
    volumes:
      - falkordbdata:/data
    ports:
      - "127.0.0.1:6380:6379"
      - "127.0.0.1:3002:3000"
    healthcheck:
      test: ["CMD", "redis-cli", "PING"]
      interval: 5s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  # ─── Backend: Fastify + QueryWeaver ───
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      postgres:
        condition: service_healthy
      falkordb:
        condition: service_healthy
      llm:
        condition: service_started
    environment:
      PORT: 8080
      NODE_ENV: production
      POSTGRES_URL: postgresql://shop_user:${POSTGRES_PASSWORD:-shop_pass_local}@postgres:5432/shop
      FALKORDB_URL: redis://falkordb:6379
      FALKOR_URL: redis://falkordb:6379
      GRAPH_NAME: shop
      OPENAI_BASE_URL: http://llm:11434/v1
      OPENAI_API_KEY: ollama
      LLM_MODEL_ID: ${LLM_MODEL_ID:-llama3.2}
      LLM_TIMEOUT_MS: ${LLM_TIMEOUT_MS:-180000}
      LLM_RETRIES: ${LLM_RETRIES:-2}
      OFFLINE_ONLY: "true"
      QUERYWEAVER_CONFIG_PATH: ./config/queryweaver.config.json
      AUDIT_LOG_DIR: /app/audit
    ports:
      - "127.0.0.1:8090:8080"
    volumes:
      - audit_logs:/app/audit
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/ping"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ─── Frontend: Wizechat UI ───
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    depends_on:
      - backend
    ports:
      - "127.0.0.1:3001:80"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:80/"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ─── Local LLM: Ollama (OpenAI-compatible, runs on CPU/Apple Silicon/GPU) ───
  llm:
    image: ollama/ollama:latest
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: unless-stopped

volumes:
  pgdata:
  falkordbdata:
  audit_logs:
  ollama_data:
